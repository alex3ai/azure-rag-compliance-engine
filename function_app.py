"""
Azure Function App - RAG Audit√°vel (v2 Programming Model)
============================================================
‚úÖ Resilience Pattern: Fallback para modo offline se LLM falhar
‚úÖ Valida√ß√£o robusta de seguran√ßa
‚úÖ Rate limiting autom√°tico
‚úÖ Auditoria completa
‚úÖ Controle de similaridade (Ajustado para 0.01 - Captura M√°xima)
"""

import azure.functions as func
import logging
import os
import json
from datetime import datetime, timedelta
from typing import Dict, Any, List, Tuple
from collections import defaultdict
import hashlib

# Importa√ß√µes de IA
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential

# Configurar logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Inicializar App
app = func.FunctionApp()

# ==================== RATE LIMITER ====================
class SimpleRateLimiter:
    """Rate limiter simples baseado em IP"""
    
    def __init__(self, max_requests: int = 20, window_minutes: int = 1):
        self.max_requests = max_requests
        self.window = timedelta(minutes=window_minutes)
        self.requests: Dict[str, List[datetime]] = defaultdict(list)
    
    def is_allowed(self, client_id: str) -> Tuple[bool, str]:
        now = datetime.now()
        # Limpar requisi√ß√µes antigas
        self.requests[client_id] = [
            req_time for req_time in self.requests[client_id]
            if now - req_time < self.window
        ]
        
        # Verificar limite
        if len(self.requests[client_id]) >= self.max_requests:
            oldest_request = min(self.requests[client_id])
            retry_after = (oldest_request + self.window - now).total_seconds()
            return False, f"Rate limit excedido. Tente novamente em {int(retry_after)}s"
        
        self.requests[client_id].append(now)
        return True, "OK"

rate_limiter = SimpleRateLimiter(max_requests=20, window_minutes=1)

# ==================== VALIDA√á√ÉO E SANITIZA√á√ÉO ====================
class InputValidator:
    """Valida e sanitiza inputs do usu√°rio"""
    
    @staticmethod
    def validate_question(question: str) -> Tuple[bool, str, str]:
        if not question:
            return False, "", "Pergunta n√£o pode ser vazia"
        if not isinstance(question, str):
            return False, "", "Pergunta deve ser texto"
        
        sanitized = question.strip()
        
        if len(sanitized) < 3:
            return False, "", "Pergunta muito curta"
        if len(sanitized) > 1000:
            return False, "", "Pergunta muito longa"
        
        return True, sanitized, ""

# ==================== GERA√á√ÉO DE RESPOSTA (COM FALLBACK) ====================
class RAGEngine:
    """Engine principal de RAG com seguran√ßa e Failover"""
    
    def __init__(self):
        # 1. Embeddings (Cr√≠tico - Deve funcionar)
        try:
            self.embeddings = AzureOpenAIEmbeddings(
                azure_deployment=os.getenv("AZURE_OPENAI_DEPLOYMENT_EMBEDDING", "text-embedding-ada-002"),
                openai_api_version="2023-05-15",
                azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
                api_key=os.getenv("AZURE_OPENAI_API_KEY")
            )
        except Exception as e:
            logger.error(f"‚ùå Falha cr√≠tica ao iniciar Embeddings: {e}")
            raise

        # 2. LLM (Opcional - Pode falhar por cota/regi√£o)
        self.llm = None
        try:
            self.llm = AzureChatOpenAI(
                azure_deployment=os.getenv("AZURE_OPENAI_DEPLOYMENT_CHAT", "gpt-35-turbo"),
                openai_api_version="2023-05-15",
                azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
                api_key=os.getenv("AZURE_OPENAI_API_KEY"),
                temperature=0,
                max_tokens=500
            )
            logger.info("‚úÖ LLM Inicializado com sucesso.")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è LLM falhou na inicializa√ß√£o (Modo Conting√™ncia Ativado): {e}")

        # 3. Search Client
        self.search_client = SearchClient(
            endpoint=os.getenv("AZURE_SEARCH_ENDPOINT"),
            index_name=os.getenv("AZURE_SEARCH_INDEX_NAME", "compliance-docs-index"),
            credential=AzureKeyCredential(os.getenv("AZURE_SEARCH_KEY"))
        )
        
        # Threshold M√≠nimo (1%) para garantir retorno do Search
        self.min_relevance_score = 0.01 
        self.top_k_chunks = 5
    
    def search_documents(self, question: str) -> List[Dict[str, Any]]:
        """Busca documentos relevantes"""
        logger.info(f"üîç Buscando documentos para: {question[:50]}...")
        
        try:
            # Gerar embedding
            question_embedding = self.embeddings.embed_query(question)
            
            # Busca H√≠brida
            results = self.search_client.search(
                search_text=question,
                vector_queries=[{
                    "kind": "vector",
                    "vector": question_embedding,
                    "k_nearest_neighbors": self.top_k_chunks,
                    "fields": "content_vector"
                }],
                select=["content", "source_file", "page_number", "compliance_level"],
                top=self.top_k_chunks
            )
            
            relevant_docs = []
            logger.info(f"--- DEBUG BUSCA: '{question}' ---")
            
            for result in results:
                score = result.get('@search.score', 0)
                source = result.get('source_file', 'Unknown')
                page = result.get('page_number', 0)
                
                logger.info(f"   üìÑ Doc: {source} (p.{page}) | Score: {score:.4f}")
                
                if score >= self.min_relevance_score:
                    relevant_docs.append({
                        'content': result.get('content', ''),
                        'source': source,
                        'page': page,
                        'compliance': result.get('compliance_level', 'UNCLASSIFIED'),
                        'relevance_score': float(score)
                    })
            
            return relevant_docs
            
        except Exception as e:
            logger.error(f"Erro na busca: {str(e)}")
            raise
    
    def generate_answer(self, question: str, docs: List[Dict[str, Any]], client_ip: str) -> Dict[str, Any]:
        """Gera resposta com Circuit Breaker (Fallback se LLM falhar)"""
        
        sources = list(set([f"{doc['source']} (p. {doc['page']})" for doc in docs]))
        
        if not docs:
            return {
                "answer": "Nenhum documento encontrado (Verifique se o PDF foi indexado corretamente).",
                "sources": [],
                "confidence": "N/A"
            }
        
        # --- TENTATIVA DE USO DO LLM ---
        try:
            if not self.llm:
                raise Exception("LLM n√£o foi inicializado corretamente.")

            logger.info("üß† Enviando prompt para o LLM...")
            
            context = "\n\n---\n\n".join([f"Fonte: {d['source']}\n{d['content']}" for d in docs])
            
            system_prompt = f"""Voc√™ √© um auditor de compliance.
            Use o contexto abaixo para responder √† pergunta. Se n√£o souber, diga "N√£o consta".
            
            CONTEXTO:
            {context}
            
            PERGUNTA: {question}"""
            
            response = self.llm.invoke(system_prompt)
            answer = response.content
            confidence_status = "ALTA"
            
        except Exception as e:
            # === MODO DE CONTING√äNCIA (FALLBACK) ===
            logger.error(f"üî• FALHA NO LLM (Ativando Fallback): {e}")
            
            # Monta resposta sint√©tica com o conte√∫do bruto
            top_content = docs[0]['content']
            source_ref = f"{docs[0]['source']} (p. {docs[0]['page']})"
            
            answer = (
                f"‚ö†Ô∏è **MODO DE CONTING√äNCIA (IA Indispon√≠vel)**\n\n"
                f"O modelo de linguagem est√° indispon√≠vel na sua regi√£o Azure (Erro de Cota/Deploy).\n"
                f"Por√©m, o sistema localizou esta informa√ß√£o relevante no documento:\n\n"
                f"\"{top_content}...\"\n\n"
                f"üìå **Fonte:** {source_ref}"
            )
            confidence_status = "CONTING√äNCIA"
            
        # Log de Auditoria
        self._log_audit(client_ip, question, sources, confidence_status)
        
        return {
            "answer": answer,
            "sources": sources,
            "confidence": confidence_status,
            "documents_used": len(docs)
        }
    
    def _log_audit(self, client_ip: str, question: str, sources: List[str], confidence: str):
        """Registra evento de auditoria"""
        audit_entry = {
            "timestamp": datetime.now().isoformat(),
            "client_ip": client_ip,
            "question_hash": hashlib.sha256(question.encode()).hexdigest()[:16],
            "sources": sources,
            "confidence": confidence
        }
        logger.info(f"AUDIT_EVENT: {json.dumps(audit_entry)}")

# Inst√¢ncia global
rag_engine = RAGEngine()
validator = InputValidator()

# ==================== ENDPOINT HTTP ====================
@app.route(route="ask_compliance", auth_level=func.AuthLevel.FUNCTION, methods=["POST"])
def ask_compliance(req: func.HttpRequest) -> func.HttpResponse:
    
    client_ip = req.headers.get('X-Forwarded-For', 'unknown')
    
    # Rate Limiting
    is_allowed, rate_msg = rate_limiter.is_allowed(client_ip)
    if not is_allowed:
        return func.HttpResponse(json.dumps({"error": rate_msg}), status_code=429)
    
    # Validar Input
    try:
        req_body = req.get_json()
        question = req_body.get('question', '')
    except ValueError:
        return func.HttpResponse(json.dumps({"error": "JSON inv√°lido"}), status_code=400)
    
    is_valid, sanitized_q, error_msg = validator.validate_question(question)
    if not is_valid:
        return func.HttpResponse(json.dumps({"error": error_msg}), status_code=400)
    
    # RAG Pipeline com Tratamento de Erro Global
    try:
        relevant_docs = rag_engine.search_documents(sanitized_q)
        result = rag_engine.generate_answer(sanitized_q, relevant_docs, client_ip)
        
        response_data = {
            **result,
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "model": "gpt-fallback" if result['confidence'] == "CONTING√äNCIA" else "gpt-standard"
            }
        }
        
        return func.HttpResponse(
            json.dumps(response_data, ensure_ascii=False, indent=2),
            mimetype="application/json",
            status_code=200
        )
        
    except Exception as e:
        logger.error(f"Erro cr√≠tico: {str(e)}", exc_info=True)
        return func.HttpResponse(
            json.dumps({"error": "Erro interno do servidor", "details": str(e)}),
            status_code=500
        )